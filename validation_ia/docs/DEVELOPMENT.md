# üõ†Ô∏è Gu√≠a de Desarrollo - Sistema de Validaci√≥n IA

Esta gu√≠a contiene informaci√≥n t√©cnica detallada para desarrolladores que trabajen en el sistema de validaci√≥n con IA Generativa.

## üóÑÔ∏è Base de Datos

### Tipo de Base de Datos
- **Motor**: PostgreSQL (a trav√©s de Supabase)
- **Versi√≥n**: PostgreSQL 15+
- **Caracter√≠sticas**: JSONB, √≠ndices GIN, triggers autom√°ticos

### Esquemas de Tablas

#### Tabla: `validation_records`
Logs individuales de validaci√≥n por producto:

```sql
CREATE TABLE validation_records (
    id SERIAL PRIMARY KEY,
    article_id TEXT NOT NULL,
    validation_date TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    status TEXT NOT NULL CHECK (status IN ('Ok', 'issues')),
    issues JSONB NOT NULL DEFAULT '[]'::jsonb,
    metadata JSONB NOT NULL DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**Campos**:
- `article_id`: ID √∫nico del producto validado
- `status`: "Ok" (sin problemas) o "issues" (con problemas)
- `issues`: Array JSON con lista de problemas detectados
- `metadata`: JSON con informaci√≥n adicional (confidence_score, suggestions, etc.)

#### Tabla: `executions`
Registro de ejecuciones completas del proceso:

```sql
CREATE TABLE executions (
    id SERIAL PRIMARY KEY,
    start_time TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    end_time TIMESTAMP WITH TIME ZONE,
    status TEXT NOT NULL CHECK (status IN ('Error', 'Done', 'Not complete', 'In Progress')),
    records_status TEXT NOT NULL CHECK (records_status IN ('Ok', 'issues', 'Not started', 'Processing')),
    issues_summary JSONB NOT NULL DEFAULT '{}'::jsonb,
    total_records INTEGER NOT NULL DEFAULT 0,
    valid_records INTEGER NOT NULL DEFAULT 0,
    invalid_records INTEGER NOT NULL DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**Estados de `status`**:
- `In Progress`: Proceso ejecut√°ndose
- `Done`: Completado exitosamente
- `Error`: Error durante la ejecuci√≥n
- `Not complete`: Interrumpido o incompleto

**Estados de `records_status`**:
- `Not started`: No se ha iniciado la validaci√≥n
- `Processing`: Validando productos
- `Ok`: Todos los productos v√°lidos
- `issues`: Algunos productos con problemas

### √çndices de Optimizaci√≥n

```sql
-- √çndices b√°sicos
CREATE INDEX idx_validation_records_article_id ON validation_records(article_id);
CREATE INDEX idx_validation_records_date ON validation_records(validation_date);
CREATE INDEX idx_validation_records_status ON validation_records(status);
CREATE INDEX idx_executions_status ON executions(status);
CREATE INDEX idx_executions_start_time ON executions(start_time);

-- √çndices GIN para JSONB (consultas eficientes en JSON)
CREATE INDEX idx_validation_records_issues_gin ON validation_records USING GIN (issues);
CREATE INDEX idx_validation_records_metadata_gin ON validation_records USING GIN (metadata);
CREATE INDEX idx_executions_issues_summary_gin ON executions USING GIN (issues_summary);
```

## üìã Modelos de Datos

### Archivo: `models/models.py`

#### Clase `Product`
Modelo principal para productos:

```python
from pydantic import BaseModel, Field
from typing import Optional

class Product(BaseModel):
    article_id: str = Field(..., description="ID √∫nico del producto")
    name: str = Field(..., description="Nombre del producto")
    original_price: float = Field(..., gt=0, description="Precio original")
    current_price: float = Field(..., gt=0, description="Precio actual")
    discount_percentage: Optional[float] = Field(None, ge=0, le=100)
    image_url: Optional[str] = None
    product_url: Optional[str] = None
    description: Optional[str] = None
    category: Optional[str] = None
    brand: Optional[str] = None
    availability: Optional[str] = None
    rating: Optional[float] = Field(None, ge=0, le=5)
    review_count: Optional[int] = Field(None, ge=0)
```

#### Validaciones Autom√°ticas
- **Precios**: Deben ser positivos (gt=0)
- **Rating**: Rango 0-5
- **Descuento**: Rango 0-100%
- **Review count**: No negativo

## üîß Gesti√≥n de Dependencias

### Archivo: `pyproject.toml`

#### Dependencias principales
```toml
[tool.poetry.dependencies]
python = "^3.10"
pydantic = "^2.0.0"      # Validaci√≥n de datos
supabase = "^2.0.0"      # Cliente de Supabase
openai = "^1.0.0"        # Cliente de OpenAI
python-dotenv = "^1.0.0" # Variables de entorno
loguru = "^0.7.3"        # Logging avanzado
```

#### Dependencias de desarrollo
```toml
[tool.poetry.group.dev.dependencies]
pytest = "^7.4.0"        # Framework de testing
pytest-cov = "^4.1.0"    # Cobertura de tests
pytest-mock = "^3.11.0"  # Mocking para tests
```

### Comandos de gesti√≥n

#### Agregar nueva dependencia
```bash
# Dependencia principal
poetry add nombre-paquete

# Dependencia de desarrollo
poetry add --group dev nombre-paquete

# Dependencia opcional
poetry add --optional nombre-paquete

# Versi√≥n espec√≠fica
poetry add "paquete==1.2.3"
```

#### Actualizar dependencias
```bash
# Actualizar todas
poetry update

# Actualizar espec√≠fica
poetry update nombre-paquete

# Ver dependencias desactualizadas
poetry show --outdated
```

## üß™ Tests Unitarios

### Estructura de Tests
```
tests/
‚îú‚îÄ‚îÄ conftest.py                    # Fixtures comunes
‚îú‚îÄ‚îÄ test_main.py                   # Tests del script principal
‚îî‚îÄ‚îÄ modules/
    ‚îú‚îÄ‚îÄ test_openia.py            # Tests del validador IA
    ‚îî‚îÄ‚îÄ test_database_connector.py # Tests del conector DB
```

### Configuraci√≥n de Pytest

#### En `pyproject.toml`:
```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-v",                    # Verbose
    "--tb=short",           # Traceback corto
    "--strict-markers",     # Marcadores estrictos
    "--disable-warnings"    # Sin warnings
]
markers = [
    "unit: Unit tests",
    "integration: Integration tests",
    "slow: Slow running tests",
    "ai: Tests that require AI/OpenAI",
    "database: Tests that require database connection"
]
```

### Comandos de Testing

#### Ejecutar todos los tests
```bash
poetry run python -m pytest --cov=tests/ --cov-report=term-missing
```

#### Tests espec√≠ficos
```bash
# Por archivo
poetry run python -m pytest tests/modules/test_openia.py

# Por funci√≥n
poetry run python -m pytest tests/test_main.py::test_validator_init

# Por marcador
poetry run python -m pytest -m unit
poetry run python -m pytest -m "not slow"

# Con output detallado
poetry run python -m pytest -v -s

# Solo tests r√°pidos
poetry run python -m pytest -m "not slow and not ai and not database"
```

#### Cobertura de c√≥digo
```bash
# Con reporte en terminal
poetry run python -m pytest --cov=modules --cov=main --cov-report=term-missing

# Con reporte HTML
poetry run python -m pytest --cov=modules --cov=main --cov-report=html

# Solo cobertura espec√≠fica
poetry run python -m pytest --cov=modules/openia --cov-report=term-missing
```

## üåç Ambiente de Desarrollo

### Crear ambiente desde cero
```bash
# 1. Clonar proyecto
git clone <repositorio>
cd validation_ia

# 2. Instalar Poetry (si no est√° instalado)
curl -sSL https://install.python-poetry.org | python3 -

# 3. Instalar dependencias
poetry install

# 4. Activar ambiente virtual
poetry shell

# 5. Verificar instalaci√≥n
poetry run python --version
poetry show
```

### Variables de entorno de desarrollo
```bash
# Copiar archivo de ejemplo
cp config.env.example .env

# Configurar variables
# .env
SUPABASE_URL=https://tu-proyecto.supabase.co
SUPABASE_SERVICE_KEY=tu-service-key
OPENAI_API_KEY=tu-openai-api-key

# Variables opcionales para desarrollo
DEBUG=true
LOG_LEVEL=DEBUG
BATCH_SIZE=5  # Lotes peque√±os para testing
```

### Configuraci√≥n de IDE

#### VSCode (`.vscode/settings.json`)
```json
{
    "python.defaultInterpreterPath": ".venv/bin/python",
    "python.testing.pytestEnabled": true,
    "python.testing.pytestArgs": ["tests"],
    "python.linting.enabled": true,
    "python.linting.flake8Enabled": true
}
```

## üöÄ Ejecuci√≥n Local

### Desarrollo con Poetry
```bash
# Activar ambiente
poetry shell

# Ejecutar aplicaci√≥n
poetry run validate

# Con par√°metros
poetry run validate --category "Electronics" --page 0

# Ejecutar con Python directamente
poetry run python main.py
```

### Desarrollo con variables de entorno
```bash
# Cargar variables y ejecutar
export $(cat .env | xargs) && python main.py

# O usando dotenv
python -c "from dotenv import load_dotenv; load_dotenv()" && python main.py
```

### Debugging
```bash
# Con pdb
poetry run python -m pdb main.py

# Con pytest debugger
poetry run python -m pytest --pdb tests/test_main.py::test_specific

# Con logs detallados
LOG_LEVEL=DEBUG poetry run validate
```

## üîç Monitoreo y Logs

### Configuraci√≥n de Loguru
```python
from loguru import logger

# Configuraci√≥n en modules/
logger.add(
    "logs/validation_{time:YYYY-MM-DD}.log",
    rotation="1 day",
    retention="30 days",
    level="INFO"
)
```

### Niveles de log
- `DEBUG`: Informaci√≥n detallada para debugging
- `INFO`: Informaci√≥n general del proceso
- `WARNING`: Situaciones que requieren atenci√≥n
- `ERROR`: Errores que no detienen la ejecuci√≥n
- `CRITICAL`: Errores cr√≠ticos que detienen el proceso

## üîß Arquitectura del C√≥digo

### Patr√≥n de Dise√±o
- **Separaci√≥n de responsabilidades**: Cada m√≥dulo tiene una funci√≥n espec√≠fica
- **Inyecci√≥n de dependencias**: Los conectores se pasan como par√°metros
- **Validaci√≥n de datos**: Pydantic para validaci√≥n autom√°tica
- **Manejo de errores**: Try-catch con fallbacks autom√°ticos

### Flujo de datos
1. `main.py` ‚Üí Script principal y CLI
2. `modules/database_connector.py` ‚Üí Comunicaci√≥n con Supabase
3. `modules/openia.py` ‚Üí Validaci√≥n con IA
4. `models/models.py` ‚Üí Definici√≥n de estructuras de datos

### Principios seguidos
- **DRY**: Don't Repeat Yourself
- **SOLID**: Especialmente Single Responsibility
- **Fail-fast**: Validaci√≥n temprana de par√°metros
- **Graceful degradation**: Fallback a validaci√≥n b√°sica si IA falla

## üìä M√©tricas y Monitoring

### M√©tricas autom√°ticas
- Tiempo de ejecuci√≥n por lote
- Porcentaje de productos v√°lidos
- Score de confianza promedio
- Errores por tipo

### Logging estructurado
```python
logger.info("Validation completed", 
    total_products=100,
    valid_products=95,
    execution_time="2.5s",
    confidence_avg=0.87
)
```

## üö® Troubleshooting

### Problemas comunes

#### Error de conexi√≥n a Supabase
```bash
# Verificar variables
echo $SUPABASE_URL
echo $SUPABASE_SERVICE_KEY

# Test de conexi√≥n
poetry run python -c "from modules.database_connector import DatabaseConnector; db = DatabaseConnector(); print('‚úÖ Conexi√≥n exitosa')"
```

#### Error de OpenAI API
```bash
# Verificar API key
echo $OPENAI_API_KEY

# Test de API
poetry run python -c "from modules.openia import AIValidator; ai = AIValidator(); print('‚úÖ OpenAI configurado')"
```

#### Tests fallando
```bash
# Instalar dependencias de desarrollo
poetry install --with dev

# Verificar pytest
poetry run python -m pytest --version

# Ejecutar test simple
poetry run python -m pytest tests/test_main.py::test_validator_init -v
```

## üéØ Mejores Pr√°cticas

### Desarrollo
1. **Siempre ejecutar tests** antes de commit
2. **Usar type hints** en todas las funciones
3. **Documentar funciones** con docstrings
4. **Validar par√°metros** con Pydantic
5. **Manejar excepciones** apropiadamente

### Performance
1. **Usar lotes** para procesamiento masivo
2. **Implementar rate limiting** para APIs externas
3. **Cachear resultados** cuando sea apropiado
4. **Optimizar consultas** de base de datos

### Seguridad
1. **Nunca commitear** archivos `.env`
2. **Usar variables de entorno** para credenciales
3. **Validar inputs** del usuario
4. **Rotar API keys** regularmente
