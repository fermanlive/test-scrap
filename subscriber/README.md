# API de Scraping - Mercado Libre Uruguay

API REST para el sistema de scraping escalable de Mercado Libre Uruguay, que recibe colas de valores y utiliza el scraper existente.

## ğŸš€ CaracterÃ­sticas

- **API REST**: Endpoints para crear y monitorear tareas de scraping
- **Cola de Tareas**: Sistema de colas usando Redis para manejar mÃºltiples solicitudes
- **Procesamiento AsÃ­ncrono**: Tareas de scraping ejecutadas en background
- **Validaciones**: ValidaciÃ³n automÃ¡tica de categorÃ­as y parÃ¡metros
- **Monitoreo**: Endpoints de salud y estadÃ­sticas del sistema
- **IntegraciÃ³n**: Utiliza el scraper existente de la carpeta `scraper`

## ğŸ“‹ Requisitos

- Python 3.8+
- Redis
- Poetry (para gestiÃ³n de dependencias)

## ğŸ› ï¸ InstalaciÃ³n

1. **Instalar dependencias con Poetry:**
   ```bash
   cd listener
   poetry install
   ```

2. **Configurar variables de entorno:**
   ```bash
   cp env.example .env
   # Editar .env con tus configuraciones
   ```

3. **Iniciar Redis:**
   ```bash
   # En macOS con Homebrew
   brew services start redis
   
   # O manualmente
   redis-server
   ```

## ğŸš€ Uso

### Iniciar la API

```bash
# Con Poetry
poetry run start-api

# O manualmente
poetry run python -m listener.main
```

### Iniciar el Worker (opcional)

```bash
# Con Poetry
poetry run start-worker

# O manualmente
poetry run python -m listener.worker
```

## ğŸ“¡ Endpoints de la API

### 1. Crear Tarea de Scraping

```http
POST /scrape
Content-Type: application/json

{
  "category": "MLU5725",
  "page": 1,
  "max_products": 20
}
```

**Respuesta:**
```json
{
  "task_id": "uuid-1234-5678",
  "status": "pending",
  "message": "Tarea de scraping creada exitosamente",
  "url": "https://www.mercadolibre.com.uy/ofertas?category=MLU5725&page=1",
  "category": "MLU5725",
  "page": 1,
  "max_products": 20
}
```

### 2. Verificar Estado de Tarea

```http
GET /tasks/{task_id}
```

**Respuesta:**
```json
{
  "id": "uuid-1234-5678",
  "request": {
    "category": "MLU5725",
    "page": 1,
    "max_products": 20
  },
  "status": "completed",
  "created_at": "2024-01-15T10:30:00",
  "started_at": "2024-01-15T10:30:05",
  "completed_at": "2024-01-15T10:32:15",
  "result_file": "/path/to/output/scraping_20240115_103215.json"
}
```

### 3. Listar Tareas

```http
GET /tasks?limit=10&offset=0
```

### 4. Verificar Salud del Sistema

```http
GET /health
```

### 5. Obtener CategorÃ­as VÃ¡lidas

```http
GET /categories
```

## ğŸ·ï¸ CategorÃ­as VÃ¡lidas

| CÃ³digo | Nombre |
|--------|--------|
| MLU5725 | Accesorios para VehÃ­culos |
| MLU1512 | Agro |
| MLU1403 | Alimentos y Bebidas |
| MLU107 | Animales y Mascotas |

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

- `API_HOST`: Host de la API (default: 0.0.0.0)
- `API_PORT`: Puerto de la API (default: 8000)
- `REDIS_HOST`: Host de Redis (default: localhost)
- `REDIS_PORT`: Puerto de Redis (default: 6379)
- `MAX_CONCURRENT_TASKS`: MÃ¡ximo de tareas concurrentes (default: 3)
- `DEFAULT_MAX_PRODUCTS`: Productos por defecto (default: 20)

### Archivos de ConfiguraciÃ³n

- `config.py`: ConfiguraciÃ³n principal de la API
- `models.py`: Modelos de datos Pydantic
- `queue_manager.py`: GestiÃ³n de colas con Redis
- `scraper_service.py`: IntegraciÃ³n con el scraper existente

## ğŸ“Š Monitoreo

### Logs

Los logs se guardan en:
- API: `logs/api.log`
- Worker: `logs/worker.log`

### MÃ©tricas

- Estado de tareas (pending, processing, completed, failed)
- EstadÃ­sticas de la cola
- Tiempo de ejecuciÃ³n
- Tasa de Ã©xito

## ğŸ”„ Flujo de Trabajo

1. **Cliente envÃ­a solicitud** â†’ `POST /scrape`
2. **API valida parÃ¡metros** â†’ CategorÃ­a, pÃ¡gina, cantidad
3. **Se crea tarea** â†’ Se guarda en Redis con estado "pending"
4. **Se ejecuta scraping** â†’ En background usando el scraper existente
5. **Se actualiza estado** â†’ "processing" â†’ "completed" o "failed"
6. **Cliente consulta estado** â†’ `GET /tasks/{task_id}`

## ğŸš¨ Manejo de Errores

- **ValidaciÃ³n de entrada**: CategorÃ­as y parÃ¡metros
- **Reintentos automÃ¡ticos**: Para fallos temporales
- **Logging detallado**: Para debugging y monitoreo
- **Estados de tarea**: Seguimiento completo del ciclo de vida

## ğŸ”’ Seguridad

- ValidaciÃ³n de entrada con Pydantic
- Rate limiting configurable
- Logs estructurados sin informaciÃ³n sensible
- Manejo seguro de errores

## ğŸ“ˆ Escalabilidad

- Cola de tareas con Redis
- Procesamiento asÃ­ncrono
- Workers independientes
- ConfiguraciÃ³n de concurrencia

## ğŸ§ª Testing

```bash
# Ejecutar tests
poetry run pytest

# Con coverage
poetry run pytest --cov=listener
```

## ğŸ“ Ejemplos de Uso

### Python con requests

```python
import requests

# Crear tarea de scraping
response = requests.post("http://localhost:8000/scrape", json={
    "category": "MLU5725",
    "page": 1,
    "max_products": 20
})

task_id = response.json()["task_id"]

# Verificar estado
status = requests.get(f"http://localhost:8000/tasks/{task_id}")
print(status.json())
```

### cURL

```bash
# Crear tarea
curl -X POST "http://localhost:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{"category": "MLU5725", "page": 1, "max_products": 20}'

# Verificar estado
curl "http://localhost:8000/tasks/{task_id}"
```

## ğŸ¤ ContribuciÃ³n

1. Fork del proyecto
2. Crear rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la misma licencia que el proyecto principal.

## ğŸ†˜ Soporte

Para soporte tÃ©cnico o preguntas:
- Crear un issue en el repositorio
- Revisar la documentaciÃ³n de la API en `/docs`
- Consultar los logs del sistema
