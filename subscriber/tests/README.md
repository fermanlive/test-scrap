# Tests Unitarios y de IntegraciÃ³n - Main.py

Este directorio contiene tests completos para `main.py` siguiendo metodologÃ­a **TDD (Test-Driven Development)** y el patrÃ³n **AAA (Arrange, Act, Assert)**.

## ğŸ“‹ Estructura de Tests

### `conftest.py`
ConfiguraciÃ³n global de fixtures y mocks para todos los tests:
- **Fixtures de modelos**: `sample_scraping_request`, `sample_scraping_task`, etc.
- **Mocks de servicios**: `mock_rabbitmq_manager`, `mock_scraper_service`
- **Configuraciones mock**: `mock_api_config`, `mock_valid_categories`

### `test_main.py`
Tests unitarios principales organizados en clases:

#### `TestMainAPI`
Tests para endpoints de la API REST:
- âœ… `test_root_endpoint_returns_correct_response`
- âœ… `test_health_check_healthy_when_all_services_available`
- âœ… `test_health_check_unhealthy_when_rabbitmq_disconnected`
- âœ… `test_health_check_unhealthy_when_scraper_unavailable`
- âœ… `test_health_check_handles_exceptions_gracefully`
- âœ… `test_listener_status_returns_correct_information`
- âœ… `test_get_categories_returns_valid_categories`
- âœ… `test_create_scraping_task_with_valid_request`
- âœ… `test_create_scraping_task_with_invalid_category`
- âœ… `test_create_scraping_task_handles_queue_manager_failure`
- âœ… `test_get_task_status_with_existing_task`
- âœ… `test_get_task_status_with_nonexistent_task`
- âœ… `test_get_task_status_handles_queue_manager_failure`
- âœ… `test_list_tasks_with_default_parameters`
- âœ… `test_list_tasks_with_custom_parameters`
- âœ… `test_list_tasks_handles_queue_manager_failure`

#### `TestBackgroundTasks`
Tests para procesamiento en background:
- âœ… `test_process_scraping_task_successful_execution`
- âœ… `test_process_scraping_task_handles_scraper_failure`

#### `TestMessageListener`
Tests para message listener:
- âœ… `test_start_message_listener_initializes_correctly`
- âœ… `test_start_message_listener_handles_exceptions`
- âœ… `test_stop_message_listener_updates_flag`

#### `TestStartupShutdown`
Tests para eventos del ciclo de vida:
- âœ… `test_startup_event_initializes_services`
- âœ… `test_startup_event_handles_initialization_failure`
- âœ… `test_shutdown_event_closes_connections`

#### `TestStartAPI`
Tests para inicializaciÃ³n de la API:
- âœ… `test_start_api_calls_uvicorn_with_correct_config`

### `test_main_integration.py`
Tests de integraciÃ³n entre componentes:

#### `TestMainIntegration`
Tests de flujos completos:
- âœ… `test_complete_scraping_workflow_integration`
- âœ… `test_health_check_integration_with_all_components`
- âœ… `test_error_handling_integration_across_components`
- âœ… `test_listener_status_integration_with_threading`
- âœ… `test_categories_integration_with_validation`
- âœ… `test_background_task_integration`

#### `TestMainPerformance`
Tests de rendimiento y carga:
- âœ… `test_multiple_concurrent_requests_performance`
- âœ… `test_health_check_response_time_consistency`
- âœ… `test_large_task_list_pagination_performance`

## ğŸ§ª MetodologÃ­a TDD

### PatrÃ³n AAA (Arrange, Act, Assert)

Cada test sigue estrictamente el patrÃ³n AAA:

```python
def test_example_following_aaa_pattern(self):
    """
    Test: DescripciÃ³n clara de lo que se estÃ¡ probando
    
    """
    # Arrange - Configurar datos y mocks necesarios
    expected_value = "expected"
    mock_service.setup_method.return_value = expected_value
    
    # Act - Ejecutar la funcionalidad que se estÃ¡ probando
    result = service_under_test.method_to_test()
    
    # Assert - Verificar que el resultado es el esperado
    assert result == expected_value
    mock_service.setup_method.assert_called_once()
```

### Ciclo TDD

1. **ğŸ”´ Red**: Escribir test que falle inicialmente
2. **ğŸŸ¢ Green**: Implementar cÃ³digo mÃ­nimo para que pase
3. **ğŸ”„ Refactor**: Mejorar el cÃ³digo manteniendo los tests verdes

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **pytest**: Framework principal de testing
- **pytest-asyncio**: Para tests asÃ­ncronos
- **FastAPI TestClient**: Para tests de endpoints
- **unittest.mock**: Para mocking y patching
- **fixtures**: Para reutilizaciÃ³n de datos de prueba

## ğŸš€ Ejecutar Tests

### Todos los tests:
```bash
poetry run pytest tests/test_main.py -v
```

### Tests especÃ­ficos:
```bash
# Solo tests unitarios
poetry run pytest tests/test_main.py::TestMainAPI -v

# Solo tests de integraciÃ³n
poetry run pytest tests/test_main_integration.py -v

# Test especÃ­fico
poetry run pytest tests/test_main.py::TestMainAPI::test_root_endpoint_returns_correct_response -v
```

### Con cobertura:
```bash
poetry run pytest tests/test_main.py --cov=main --cov-report=html
```

### En modo verbose:
```bash
poetry run pytest tests/test_main.py -v -s
```

## ğŸ“Š Cobertura de Tests

Los tests cubren:

### âœ… Endpoints HTTP
- `GET /` - Endpoint raÃ­z
- `GET /health` - Health check
- `GET /listener/status` - Estado del listener
- `POST /scrape` - Crear tarea de scraping
- `GET /tasks/{task_id}` - Obtener tarea especÃ­fica
- `GET /tasks` - Listar tareas
- `GET /categories` - Obtener categorÃ­as

### âœ… Funciones de Background
- `process_scraping_task` - Procesamiento de tareas
- `start_message_listener` - Iniciar listener
- `stop_message_listener` - Detener listener

### âœ… Eventos del Ciclo de Vida
- `startup_event` - InicializaciÃ³n de servicios
- `shutdown_event` - Cierre de conexiones
- `start_api` - InicializaciÃ³n con uvicorn

### âœ… Manejo de Errores
- Errores de validaciÃ³n (400)
- Recursos no encontrados (404)
- Errores internos (500)
- Excepciones en servicios

### âœ… Casos Edge
- Servicios no disponibles
- Fallos de conectividad
- Datos invÃ¡lidos
- Condiciones de concurrencia

## ğŸ”§ Mocks y Fixtures

### Principales Mocks:
- **RabbitMQManager**: GestiÃ³n de colas
- **ScraperService**: Servicio de scraping
- **MessageListener**: Listener de mensajes
- **Threading**: Sistema de hilos
- **UUID**: GeneraciÃ³n de IDs Ãºnicos

### Fixtures Reutilizables:
- `sample_scraping_request`: Request vÃ¡lido
- `sample_scraping_task`: Tarea de ejemplo
- `mock_api_config`: ConfiguraciÃ³n de API
- `mock_valid_categories`: CategorÃ­as vÃ¡lidas

## ğŸ“ˆ Beneficios de esta Estructura

1. **Cobertura Completa**: Cubre todos los endpoints y funcionalidades
2. **Mantenibilidad**: Tests organizados y bien documentados
3. **ReutilizaciÃ³n**: Fixtures y mocks reutilizables
4. **Aislamiento**: Cada test es independiente
5. **Performance**: Tests de rendimiento incluidos
6. **IntegraciÃ³n**: Tests que verifican interacciones entre componentes
7. **DocumentaciÃ³n**: Cada test documenta el comportamiento esperado

## ğŸ” Casos de Uso Cubiertos

- âœ… CreaciÃ³n exitosa de tareas de scraping
- âœ… ValidaciÃ³n de categorÃ­as invÃ¡lidas
- âœ… Manejo de errores del queue manager
- âœ… VerificaciÃ³n del estado de salud del sistema
- âœ… Procesamiento en background de tareas
- âœ… GestiÃ³n del ciclo de vida de la aplicaciÃ³n
- âœ… Manejo de fallos en servicios externos
- âœ… PaginaciÃ³n de listas grandes
- âœ… Requests concurrentes
- âœ… IntegraciÃ³n entre componentes
